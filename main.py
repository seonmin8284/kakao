from fastapi import FastAPI, Request, BackgroundTasks
from fastapi.responses import JSONResponse
import openai
from dotenv import load_dotenv
import os
from typing import Dict, Any
import uuid

# í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ
load_dotenv()
openai.api_key = os.getenv("OPENAI_API_KEY")

app = FastAPI()

# ì €ì¥ì†Œ (ì‹¤ì œ ìš´ì˜ì—ì„œëŠ” DBë‚˜ Redis ì‚¬ìš©)
GPT_RESPONSES: Dict[str, str] = {}
USER_INPUTS: Dict[str, str] = {}
USER_SLOT_STATE: Dict[str, Dict[str, str]] = {}

# ì„œë¹„ìŠ¤ ì¹´í…Œê³ ë¦¬ ë°ì´í„°
SERVICE_CATEGORIES = {
    "ì‹œê°í™”_ëŒ€ì‹œë³´ë“œ": {
        "ê¸°íš_ìš”êµ¬ì‚¬í•­_ì •ì˜": {
            "features": ["ë¶„ì„ ëª©ì  ë° ì£¼ìš” KPI ì •ì˜", "ì‚¬ìš©ì ìš”êµ¬ ì •ë¦¬", "ë°ì´í„° ì‹œê°í™” ë°©í–¥ ìˆ˜ë¦½"],
            "cost": 400000
        },
        "ë°ì´í„°_ìˆ˜ì§‘_ì „ì²˜ë¦¬": {
            "features": ["ë‚´ë¶€/ì™¸ë¶€ ë°ì´í„° ìˆ˜ì§‘", "ë°ì´í„° ì •ì œ ë° ê°€ê³µ", "Power BI/Tableau ì ì¬"],
            "cost": 800000
        },
        "ëŒ€ì‹œë³´ë“œ_í”„ë¡œí† íƒ€ì…_ì œì‘": {
            "features": ["í•µì‹¬ KPI ìœ„ì£¼ì˜ ì‹œê°í™” ëª¨ë“ˆ ê°œë°œ", "í”¼ë“œë°± ë°˜ì˜ êµ¬ì¡° êµ¬ì„±"],
            "cost": 1000000
        },
        "ì‚¬ìš©ì_ë§ì¶¤í˜•_ê¸°ëŠ¥_ì¶”ê°€": {
            "features": ["í•„í„°", "Drill-Down", "ê¶Œí•œë³„ ë·°", "ì£¼ê°„/ì›”ê°„ ë¦¬í¬íŠ¸ ìë™í™”"],
            "cost": 1000000
        },
        "ìë™í™”_ìš´ì˜_ì—°ë™": {
            "features": ["ë°ì´í„° ìë™ ì—…ë°ì´íŠ¸", "ë°°ì¹˜ ìŠ¤ì¼€ì¤„ë§(Airflow)", "ì•Œë¦¼/ë¦¬í¬íŠ¸ ìë™í™”"],
            "cost": 1000000
        }
    },
    "AI_ì±—ë´‡": {
        "ê¸°íš_ì¡°ì‚¬": {
            "features": ["ìš”êµ¬ì‚¬í•­ ë¶„ì„", "ìœ ì¦ˆì¼€ì´ìŠ¤ ì •ì˜", "ê²½ìŸ ë¶„ì„", "AI í™œìš©ë°©ì•ˆ ì„¤ê³„"],
            "cost": 500000
        },
        "ë°ì´í„°_ìˆ˜ì§‘_ì „ì²˜ë¦¬": {
            "features": ["í¬ë¡¤ë§", "ì •ì œ", "ë ˆì´ë¸”ë§", "í† í¬ë‚˜ì´ì§•", "ì´ë¯¸ì§€/ìŒì„±/í…ìŠ¤íŠ¸ ë°ì´í„°ì…‹ êµ¬ì„±"],
            "cost": 500000
        },
        "AI_ëª¨ë¸_ê°œë°œ": {
            "APIê³ ë„í™”": {
                "features": ["ìŒì„±/ì´ë¯¸ì§€/ì–¸ì–´ ìƒì„± ë˜ëŠ” ì¸ì‹ ëª¨ë¸ ê°œë°œ", "í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§"],
                "cost": 2000000
            },
            "íŒŒì¸íŠœë‹": {
                "features": ["ì˜¤í”ˆì†ŒìŠ¤ LLM (LLaMA, Mistral ë“±) Fine-tuning", "ë°ì´í„° ê¸°ë°˜ í•™ìŠµ ìŠ¤í¬ë¦½íŠ¸ êµ¬ì„±"],
                "cost": 3000000
            }
        },
        "ëª¨ë¸_í‰ê°€_ê°œì„ ": {
            "features": ["ì •í™•ë„/ì •ë°€ë„/F1 Score", "ì‹¤ì œ QA ì‹œë‚˜ë¦¬ì˜¤ ì„±ëŠ¥ í‰ê°€", "ì‹¤íŒ¨ ì¼€ì´ìŠ¤ ë¶„ì„"],
            "cost": 1500000
        },
        "í”Œë«í¼_MVP_êµ¬í˜„": {
            "features": ["ë°±ì—”ë“œ API", "ì±—ë´‡ UI(ì›¹/ì•±)", "ì¸ì¦/ê¶Œí•œ ì‹œìŠ¤í…œ", "ëŒ€í™” íë¦„ êµ¬í˜„"],
            "cost": 3000000
        },
        "ìš´ì˜_ìë™í™”_ëª¨ë‹ˆí„°ë§": {
            "features": ["ëª¨ë¸ ì¬í•™ìŠµ íŒŒì´í”„ë¼ì¸", "ë¡œê·¸ ìˆ˜ì§‘", "ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ëŒ€ì‹œë³´ë“œ"],
            "cost": 1000000
        }
    },
    "ë°ì´í„°_ì—”ì§€ë‹ˆì–´ë§": {
        "ìš”êµ¬ì‚¬í•­_ì •ì˜_ì„¤ê³„": {
            "features": ["ìˆ˜ì§‘ ëŒ€ìƒ ì •ì˜", "ìŠ¤í‚¤ë§ˆ ì„¤ê³„", "íŒŒì´í”„ë¼ì¸ êµ¬ì¡° ì„¤ê³„"],
            "cost": 500000
        },
        "ë°ì´í„°_ìˆ˜ì§‘_ëª¨ë“ˆ_ê°œë°œ": {
            "features": ["Public API", "ì›¹ í¬ë¡¤ë§", "DB ì¶”ì¶œ ë“± ë°ì´í„° ìˆ˜ì§‘ ìë™í™” êµ¬í˜„"],
            "cost": 500000
        },
        "ë°ì´í„°_ì²˜ë¦¬_ì •ì œ": {
            "features": ["ê²°ì¸¡ì¹˜ ì²˜ë¦¬", "ì¤‘ë³µ ì œê±°", "í¬ë§· ë³€í™˜", "ì»¬ëŸ¼ ì •ë¦¬ ë“± ì „ì²˜ë¦¬ ë¡œì§ ê°œë°œ"],
            "cost": 500000
        },
        "ì €ì¥_ì ì¬_ìë™í™”": {
            "features": ["ì •ì œ ë°ì´í„°ì˜ ì €ì¥ (SQL, Data Lake, Warehouse ë“±) ë° ë²„ì „ ê´€ë¦¬"],
            "cost": 500000
        },
        "íŒŒì´í”„ë¼ì¸_ìë™í™”": {
            "features": ["Apache Airflow", "Python ìŠ¤ì¼€ì¤„ëŸ¬", "CI/CD ë“± í™œìš©í•œ ìë™í™” êµ¬ì„±"],
            "cost": 700000
        },
        "ëª¨ë‹ˆí„°ë§_ì˜¤ë¥˜_ì•Œë¦¼": {
            "features": ["ì‹¤íŒ¨ ë¡œê·¸ ìˆ˜ì§‘", "ì‘ì—… ì„±ê³µ ì—¬ë¶€ ì‹œê°í™”", "ìŠ¬ë™/ë©”ì¼ ì•Œë¦¼ ì—°ë™"],
            "cost": 500000
        }
    },
    "ì›¹_í”Œë«í¼": {
        "ê¸°íš_ìš”êµ¬ì‚¬í•­_ì •ì˜": {
            "features": ["ê³ ê° ìš”êµ¬ì‚¬í•­ ë¶„ì„", "í•µì‹¬ ê¸°ëŠ¥ ë„ì¶œ", "ê²½ìŸ ë²¤ì¹˜ë§ˆí‚¹", "í”Œë«í¼ êµ¬ì¡° ì„¤ê³„", "ê¸°ìˆ  ìŠ¤íƒ ì„ ì •"],
            "cost": 1000000
        },
        "í”„ë¡ íŠ¸ì—”ë“œ_ê°œë°œ": {
            "features": ["ì‚¬ìš©ì UI/UX êµ¬í˜„ (React/Vue ê¸°ë°˜)", "ë°˜ì‘í˜• ë””ìì¸ ì ìš©"],
            "cost": 2000000
        },
        "ë°±ì—”ë“œ_ê°œë°œ": {
            "features": ["API ì„œë²„", "ì¸ì¦ ì‹œìŠ¤í…œ", "DB ì—°ë™", "ì•Œë¦¼ ì‹œìŠ¤í…œ ë“± êµ¬í˜„"],
            "cost": 3000000
        },
        "ìš´ì˜ì_ê´€ë¦¬_ì‹œìŠ¤í…œ": {
            "features": ["ê´€ë¦¬ì í˜ì´ì§€", "ê¶Œí•œ ê´€ë¦¬", "ì‚¬ìš©ì/ë°ì´í„° ëª¨ë‹ˆí„°ë§ ê¸°ëŠ¥"],
            "cost": 2000000
        },
        "ë°°í¬_í†µí•©_ìœ ì§€ë³´ìˆ˜": {
            "features": ["ë„ë©”ì¸ ì—°ë™", "ì„œë²„ ë°°í¬", "ì´ˆê¸° ì˜¤ë¥˜ ëŒ€ì‘ ë° ìœ ì§€ë³´ìˆ˜ ê°€ì´ë“œ ì œê³µ"],
            "cost": 1000000
        }
    }
}

def build_prompt(user_input: str, service_categories: Dict[str, Any]) -> str:
    """ì‚¬ìš©ì ì…ë ¥ê³¼ ì„œë¹„ìŠ¤ ì¹´í…Œê³ ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ GPT í”„ë¡¬í”„íŠ¸ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
    prompt = f"ì‚¬ìš©ìì˜ ìš”ì²­:\n\"{user_input}\"\n\n"
    prompt += "ìš°ë¦¬ íšŒì‚¬ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì„œë¹„ìŠ¤ ì¹´í…Œê³ ë¦¬ë¥¼ ì œê³µí•©ë‹ˆë‹¤:\n"

    for category, steps in service_categories.items():
        prompt += f"\nğŸ“‚ {category.replace('_', ' ')}\n"
        for step, content in steps.items():
            if isinstance(content, dict) and "features" in content:
                cost = content.get("cost", 0)
                features = " / ".join(content["features"])
                prompt += f"  - {step.replace('_', ' ')}: {features} (ë¹„ìš©: {cost:,}ì›)\n"
            elif isinstance(content, dict):
                for substep, subcontent in content.items():
                    if "features" in subcontent:
                        cost = subcontent.get("cost", 0)
                        features = " / ".join(subcontent["features"])
                        prompt += f"  - {step.replace('_', ' ')} > {substep}: {features} (ë¹„ìš©: {cost:,}ì›)\n"

    prompt += "\në‹¤ìŒ í˜•ì‹ìœ¼ë¡œ ë‹µë³€í•´ ì£¼ì„¸ìš”:\n"
    prompt += "1. ì¶”ì²œ ì„œë¹„ìŠ¤: ì‚¬ìš©ìì˜ ìš”êµ¬ì‚¬í•­ì— ê°€ì¥ ì í•©í•œ ì¹´í…Œê³ ë¦¬\n"
    prompt += "2. í•„ìš”í•œ ë‹¨ê³„: ê° ë‹¨ê³„ë³„ ì£¼ìš” ê¸°ëŠ¥ê³¼ ë¹„ìš©\n"
    prompt += "3. ì˜ˆìƒ ê¸°ê°„: ì „ì²´ í”„ë¡œì íŠ¸ ì†Œìš” ê¸°ê°„\n"
    prompt += "4. ì´ ê²¬ì : ëª¨ë“  ë‹¨ê³„ì˜ ë¹„ìš© í•©ê³„\n"
    prompt += "5. ì¶”ê°€ ê³ ë ¤ì‚¬í•­: ì„ íƒì ìœ¼ë¡œ ì¶”ê°€í•  ìˆ˜ ìˆëŠ” ê¸°ëŠ¥ì´ë‚˜ ëŒ€ì•ˆ\n"
    
    return prompt

def call_gpt_for_estimate(user_input: str) -> str:
    """GPT APIë¥¼ í˜¸ì¶œí•˜ì—¬ ê²¬ì  ì‘ë‹µì„ ìƒì„±í•©ë‹ˆë‹¤."""
    try:
        prompt = build_prompt(user_input, SERVICE_CATEGORIES)
        
        response = openai.ChatCompletion.create(
            model="gpt-3.5-turbo",  # ë˜ëŠ” "gpt-3.5-turbo"
            messages=[
                {
                    "role": "system", 
                    "content": "ë‹¹ì‹ ì€ IT í”„ë¡œì íŠ¸ ê²¬ì  ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì¹œì ˆí•˜ê³  ì „ë¬¸ì ìœ¼ë¡œ ìƒë‹´í•´ì£¼ì„¸ìš”. ì´ëª¨ì§€ë¥¼ ì ì ˆíˆ í™œìš©í•˜ì—¬ ë‹µë³€í•˜ë˜, í˜•ì‹ì€ ë°˜ë“œì‹œ ì§€ì •ëœ ëŒ€ë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”."
                },
                {"role": "user", "content": prompt}
            ],
            temperature=0.7,
            max_tokens=700
        )
        return response.choices[0].message.content
    except Exception as e:
        return f"âš ï¸ ì£„ì†¡í•©ë‹ˆë‹¤. ê²¬ì  ì‚°ì¶œ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì˜¤ë¥˜ ë‚´ìš©: {str(e)}"

# ë¹„ë™ê¸° GPT ìš”ì²­ ì²˜ë¦¬
async def process_gpt(user_id: str, user_input: str):
    USER_INPUTS[user_id] = user_input
    GPT_RESPONSES[user_id] = "â³ ìš”ì²­ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤. ì ì‹œë§Œ ê¸°ë‹¤ë ¤ì£¼ì„¸ìš”..."
    GPT_RESPONSES[user_id] = call_gpt_for_estimate(user_input)

@app.post("/kakao/webhook")
async def kakao_webhook(request: Request, background_tasks: BackgroundTasks):
    """ì¹´ì¹´ì˜¤í†¡ ì›¹í›… ì—”ë“œí¬ì¸íŠ¸"""
    try:
        body = await request.json()
        user_id = body.get("userRequest", {}).get("user", {}).get("id", str(uuid.uuid4()))
        utterance = body.get("userRequest", {}).get("utterance", "")
        
        # ê²¬ì  ê²°ê³¼ í™•ì¸ ìš”ì²­ ì²˜ë¦¬
        if utterance.startswith("ê²¬ì  ê²°ê³¼ í™•ì¸:"):
            result_user_id = utterance.split("ê²¬ì  ê²°ê³¼ í™•ì¸:")[-1].strip()
            return await get_result(result_user_id)
            
        # ìƒˆë¡œìš´ ê²¬ì  ë¬¸ì˜ ì‹œ ìƒíƒœ ì´ˆê¸°í™”
        if utterance == "ìƒˆë¡œìš´ ê²¬ì  ë¬¸ì˜":
            USER_SLOT_STATE.pop(user_id, None)
            USER_INPUTS.pop(user_id, None)
            GPT_RESPONSES.pop(user_id, None)
        
        # ì²˜ë¦¬ ê°€ëŠ¥ ì—¬ë¶€ í™•ì¸
        if not any(keyword in utterance for keyword in ["í¬íŠ¸í´ë¦¬ì˜¤", "ê°€ê²©", "ê²¬ì ", "ë¹„ìš©", "í”„ë¡œì íŠ¸", "ê°œë°œ", "ì œì‘"]):
            return JSONResponse(content={
                "version": "2.0",
                "template": {
                    "outputs": [{
                        "simpleText": {
                            "text": "ì£„ì†¡í•©ë‹ˆë‹¤. ì €ëŠ” í¬íŠ¸í´ë¦¬ì˜¤ í™•ì¸ê³¼ ê²¬ì  ìƒë‹´ë§Œ ë„ì™€ë“œë¦´ ìˆ˜ ìˆì–´ìš”. ğŸ˜…\n\në‹¤ìŒê³¼ ê°™ì€ ë‚´ìš©ì„ ë¬¸ì˜í•´ì£¼ì„¸ìš”:\n- í”„ë¡œì íŠ¸ ê²¬ì  ë¬¸ì˜\n- í¬íŠ¸í´ë¦¬ì˜¤ í™•ì¸\n- ê°œë°œ ë¹„ìš© ìƒë‹´"
                        }
                    }],
                    "quickReplies": [{
                        "messageText": "ìƒˆë¡œìš´ ê²¬ì  ë¬¸ì˜",
                        "action": "message",
                        "label": "ê²¬ì  ë¬¸ì˜í•˜ê¸°"
                    }]
                }
            })
        
        # íŒŒë¼ë¯¸í„° ì¶”ì¶œ (ìƒì„¸ íŒŒë¼ë¯¸í„° ìš°ì„ , ì—†ìœ¼ë©´ ì¼ë°˜ íŒŒë¼ë¯¸í„° ì‚¬ìš©)
        params = body.get("action", {}).get("params", {})
        detail_params = body.get("action", {}).get("detailParams", {})
        
        print("[DEBUG] params:", params)
        print("[DEBUG] detail_params:", detail_params)
        
        # ê¸°ì¡´ ìƒíƒœ ì—†ìœ¼ë©´ ì´ˆê¸°í™”
        if user_id not in USER_SLOT_STATE:
            USER_SLOT_STATE[user_id] = {"ì£¼ì œ": "", "ì‚°ì¶œë¬¼": "", "ê¸°ê°„": "", "retry_count": 0}
            
        # ë‹¨ì–´ ê¸°ë°˜ ìŠ¬ë¡¯ ì¶”ë¡ 
        if USER_SLOT_STATE[user_id]["ì£¼ì œ"] == "" and any(keyword in utterance for keyword in ["í”Œë«í¼", "ì›¹", "ì•±", "ì‹œìŠ¤í…œ", "ì‚¬ì´íŠ¸"]):
            USER_SLOT_STATE[user_id]["ì£¼ì œ"] = utterance
        elif USER_SLOT_STATE[user_id]["ì‚°ì¶œë¬¼"] == "" and any(keyword in utterance for keyword in ["ì±—ë´‡", "ëŒ€ì‹œë³´ë“œ", "API", "ì„œë²„", "ë°ì´í„°ë² ì´ìŠ¤"]):
            USER_SLOT_STATE[user_id]["ì‚°ì¶œë¬¼"] = utterance
        elif USER_SLOT_STATE[user_id]["ê¸°ê°„"] == "" and any(keyword in utterance for keyword in ["ì¼", "ê°œì›”", "ì£¼", "ë‹¬", "ë…„"]):
            USER_SLOT_STATE[user_id]["ê¸°ê°„"] = utterance
            
        # íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ (ìƒì„¸ íŒŒë¼ë¯¸í„° ìš°ì„ , ì¼ë°˜ íŒŒë¼ë¯¸í„°, ë°œí™” ìˆœ)
        for slot in ["ì£¼ì œ", "ì‚°ì¶œë¬¼", "ê¸°ê°„"]:
            if slot in detail_params and detail_params[slot].get("origin"):
                USER_SLOT_STATE[user_id][slot] = detail_params[slot]["origin"]
            elif slot in params:
                USER_SLOT_STATE[user_id][slot] = params.get(slot) or params.get(f"${slot}", "")
            elif USER_SLOT_STATE[user_id][slot] == "":  # ì•„ì§ë„ ë¹„ì–´ìˆìœ¼ë©´
                # ì´ì „ì— í•´ë‹¹ ìŠ¬ë¡¯ì„ ìš”ì²­í–ˆì—ˆë‹¤ë©´, í˜„ì¬ ë°œí™”ë¥¼ í•´ë‹¹ ìŠ¬ë¡¯ì˜ ê°’ìœ¼ë¡œ ì‚¬ìš©
                last_requested_slot = USER_SLOT_STATE[user_id].get("last_requested_slot")
                if last_requested_slot == slot:
                    USER_SLOT_STATE[user_id][slot] = utterance
                
        user_state = USER_SLOT_STATE[user_id]
        
        # ë¯¸ì…ë ¥ëœ ìŠ¬ë¡¯ í™•ì¸
        missing_slots = [k for k, v in user_state.items() if not v and k != "last_requested_slot" and k != "retry_count"]
        
        if missing_slots:
            # retry_count ì¦ê°€
            USER_SLOT_STATE[user_id]["retry_count"] += 1
            
            # 3íšŒ ì´ìƒ ì‹¤íŒ¨ ì‹œ ì „ì²´ ì´ˆê¸°í™”
            if USER_SLOT_STATE[user_id]["retry_count"] >= 3:
                USER_SLOT_STATE.pop(user_id, None)
                USER_INPUTS.pop(user_id, None)
                GPT_RESPONSES.pop(user_id, None)
                
                return JSONResponse(content={
                    "version": "2.0",
                    "template": {
                        "outputs": [{
                            "simpleText": {
                                "text": "âš ï¸ ì—¬ëŸ¬ ë²ˆ ì •ë³´ë¥¼ ì •í™•íˆ ë°›ì§€ ëª»í–ˆì–´ìš”. ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì§„í–‰í•´ ì£¼ì„¸ìš”!"
                            }
                        }],
                        "quickReplies": [{
                            "messageText": "ìƒˆë¡œìš´ ê²¬ì  ë¬¸ì˜",
                            "action": "message",
                            "label": "ì²˜ìŒë¶€í„° ë‹¤ì‹œ"
                        }]
                    }
                })
            
            # ì•„ì§ 3íšŒ ë¯¸ë§Œì´ë©´ ë‹¤ìŒ ìŠ¬ë¡¯ ìš”ì²­
            next_slot = missing_slots[0]
            USER_SLOT_STATE[user_id]["last_requested_slot"] = next_slot
            return JSONResponse(content={
                "version": "2.0",
                "template": {
                    "outputs": [{
                        "simpleText": {
                            "text": f"'{next_slot}' ì •ë³´ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”! ({USER_SLOT_STATE[user_id]['retry_count']}íšŒ ì‹œë„ë¨)"
                        }
                    }]
                }
            })
            
        # ëª¨ë“  ì •ë³´ê°€ ì…ë ¥ë˜ì—ˆì„ ê²½ìš°
        full_input = f"""
í”„ë¡œì íŠ¸ ì£¼ì œ: {user_state['ì£¼ì œ']}
ì‚°ì¶œë¬¼: {user_state['ì‚°ì¶œë¬¼']}
ì˜ˆìƒ ê¸°ê°„: {user_state['ê¸°ê°„']}
        """.strip()
        
        # GPT ìš”ì²­ ë¹„ë™ê¸° ì²˜ë¦¬
        background_tasks.add_task(process_gpt, user_id, full_input)
        
        return JSONResponse(content={
            "version": "2.0",
            "template": {
                "outputs": [{
                    "simpleText": {
                        "text": f"ğŸ“ ëª¨ë“  ì •ë³´ë¥¼ ë°›ì•˜ì–´ìš”! ëª‡ ì´ˆ í›„ ê²°ê³¼ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.\n\ní˜„ì¬ ì…ë ¥ëœ ì •ë³´:\n{full_input}\n\nğŸ‘‰ í™•ì¸: /result/{user_id}"
                    }
                }],
                "quickReplies": [{
                    "messageText": f"ê²¬ì  ê²°ê³¼ í™•ì¸:{user_id}",
                    "action": "message",
                    "label": "ê²¬ì  ê²°ê³¼ í™•ì¸"
                }]
            }
        })
    except Exception as e:
        return JSONResponse(content={
            "version": "2.0",
            "template": {
                "outputs": [{
                    "simpleText": {
                        "text": f"âš ï¸ ìš”ì²­ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.\nì ì‹œ í›„ ë‹¤ì‹œ ì‹œë„í•´ì£¼ì„¸ìš”.\n\nì˜¤ë¥˜ ë‚´ìš©: {str(e)}"
                    }
                }]
            }
        })

@app.get("/result/{user_id}")
async def get_result(user_id: str):
    """ê²°ê³¼ ì¡°íšŒ ì—”ë“œí¬ì¸íŠ¸"""
    response_text = GPT_RESPONSES.get(user_id, "âŒ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìš”ì²­ IDì´ê±°ë‚˜ ì•„ì§ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤.")
    user_input = USER_INPUTS.get(user_id, "ì…ë ¥ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    # ê²°ê³¼ ì¡°íšŒ í›„ ìƒíƒœ ì´ˆê¸°í™” (ì„ íƒì‚¬í•­)
    USER_SLOT_STATE.pop(user_id, None)
    
    return {
        "version": "2.0",
        "template": {
            "outputs": [{
                "simpleText": {
                    "text": f"{response_text}\n\nğŸ—‚ï¸ ì…ë ¥ ì •ë³´:\n{user_input}"
                }
            }],
            "quickReplies": [{
                "messageText": "ìƒˆë¡œìš´ ê²¬ì  ë¬¸ì˜",
                "action": "message",
                "label": "ìƒˆë¡œìš´ ê²¬ì  ë¬¸ì˜"
            }]
        }
    }

@app.get("/health")
async def health_check():
    """í—¬ìŠ¤ ì²´í¬ ì—”ë“œí¬ì¸íŠ¸"""
    return {"status": "healthy"}
